{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "logisticRegression_FastText.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshala/Assembler/blob/master/logisticRegression_FastText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81A5G3tN27o6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from scipy.stats.stats import pearsonr\n",
        "from scipy.stats.stats import spearmanr\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from gensim.models import FastText\n",
        "import matplotlib.cm as cm\n",
        "from tempfile import TemporaryFile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSxF9QUS27pL",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "ef1305e0-b130-4fb8-8160-13f39ff46966"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9ab3334f-c2d5-478b-bead-108bda3bb22d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9ab3334f-c2d5-478b-bead-108bda3bb22d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving each_day_label_text.csv to each_day_label_text.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJzM5_Uj3hDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('each_day_label_text.csv')\n",
        "df = df.dropna()\n",
        "# df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K0imN973iW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "767ccb0f-d796-420a-baa7-5516683c6932"
      },
      "source": [
        "df['label'].loc[(df['label'] == 'Shock')] = 1\n",
        "df['label'].loc[(df['label'] == 'Non-Shock')] = 0\n",
        "# df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNYlv6yg3kXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "47a8d7e4-1474-4f41-dfaa-ef6b63b4c918"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW_TE1No4rRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oS1BbF55oUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Making list of sentences after\n",
        "sentences = []\n",
        "for text in df[\"text\"]:\n",
        "   #1. Replace \"\\n\" with \" \" (spaces)\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   text = text.replace(\"\\t\", \" \")\n",
        "   #2. Replace \"[** - **]\" data in this format with \"\" (mostly time and date)\n",
        "   text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', \"\", text)\n",
        "   #3. Removed wide spaces\n",
        "   text = re.sub(' +', \" \", text)\n",
        "   sentences.append(sent_tokenize(text))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix2Z83nJ57OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "for sen in sentences:\n",
        "    patient_words = []\n",
        "    for s in sen :\n",
        "        s_sep = \"\"\n",
        "        temp = []\n",
        "        for character in s:\n",
        "            if character in punctuations:\n",
        "                outputCharacter = \" %s \" % character\n",
        "            else:\n",
        "                outputCharacter = character\n",
        "            s_sep += outputCharacter\n",
        "        # Tokenize sentences to word tokens\n",
        "        for wrd in TreebankWordTokenizer().tokenize(s_sep):\n",
        "            # Removed stop words and punctations\n",
        "            if wrd not in punctuations:\n",
        "                #6. Converted all the words to lower case\n",
        "                wrd_lower = wrd.lower()\n",
        "                #wrd_lower = lancaster.stem(wrd_lower)\n",
        "                if ((wrd_lower not in stop_words) & (not wrd_lower.isdigit()) & (len(wrd_lower)>1) & (wrd_lower != '\\ufeff' )):\n",
        "                    temp.append(wrd_lower)\n",
        "        patient_words.extend(temp)\n",
        "    words.append(patient_words)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMRdRrt0kcEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import FastText\n",
        "model = FastText(words, min_count = 1, size = 300, window = 5, sg = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF9A4sXSmi_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model['sepsis']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r0-OWpqtzwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sif_embeddings(sentences, model, alpha=1e-3):\n",
        "    \"\"\"Compute the SIF embeddings for a list of sentences\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentences : list\n",
        "        The sentences to compute the embeddings for\n",
        "    model : `~gensim.models.base_any2vec.BaseAny2VecModel`\n",
        "        A gensim model that contains the word vectors and the vocabulary\n",
        "    alpha : float, optional\n",
        "        Parameter which is used to weigh each individual word based on its probability p(w).\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray \n",
        "        SIF sentence embedding matrix of dim len(sentences) * dimension\n",
        "    \"\"\"\n",
        "    \n",
        "    vlookup = model.wv.vocab  # Gives us access to word index and count\n",
        "    vectors = model.wv        # Gives us access to word vectors\n",
        "    size = model.vector_size  # Embedding size\n",
        "    \n",
        "    Z = 0\n",
        "    for k in vlookup:\n",
        "        Z += vlookup[k].count # Compute the normalization constant Z\n",
        "    \n",
        "    output = []\n",
        "    \n",
        "    # Iterate all sentences\n",
        "    for s in sentences:\n",
        "        count = 0\n",
        "        v = np.zeros(size, dtype=REAL) # Summary vector\n",
        "        # Iterare all words\n",
        "        for w in s:\n",
        "            # A word must be present in the vocabulary\n",
        "            if w in vlookup:\n",
        "                for i in range(size):\n",
        "                    v[i] += ( alpha / (alpha + (vlookup[w].count / Z))) * vectors[w][i]\n",
        "                count += 1 \n",
        "                \n",
        "        if count > 0:\n",
        "            for i in range(size):\n",
        "                v[i] *= 1/count\n",
        "        output.append(v)\n",
        "    return np.vstack(output).astype(REAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGTC-5L0v348",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3df8609-11af-4bf5-a088-e221fb70897a"
      },
      "source": [
        "REAL = np.float32 \n",
        "array = sif_embeddings(words, model, 1e-3)\n",
        "array.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2569, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRg2PNsSpWoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "9ddf4c76-7e85-40a0-eb64-4b887ca49adb"
      },
      "source": [
        "array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.03017591,  0.03076372, -0.03969172, ..., -0.00711262,\n",
              "        -0.06732748,  0.02095205],\n",
              "       [-0.00549397,  0.01410158, -0.05221346, ...,  0.01477716,\n",
              "        -0.06164019,  0.02998966],\n",
              "       [-0.01308691,  0.01827761, -0.05502955, ...,  0.00611987,\n",
              "        -0.06180539,  0.0424997 ],\n",
              "       ...,\n",
              "       [-0.03443385,  0.05112674, -0.05015321, ..., -0.01269269,\n",
              "        -0.06895726,  0.04280884],\n",
              "       [-0.01794074,  0.0547568 , -0.03875435, ..., -0.00679507,\n",
              "        -0.05527372,  0.03846744],\n",
              "       [-0.0308524 ,  0.06115716, -0.05419201, ..., -0.00519136,\n",
              "        -0.06308854,  0.05395428]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeL9JQ8Ffj5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt('FastText_embeddings.txt', array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFH0vM6EfmE_",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "8143a26e-ee0d-44fb-ed0f-20ff69327f11"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-169fb9f9-bd5c-428f-9d77-d21e9633f594\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-169fb9f9-bd5c-428f-9d77-d21e9633f594\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving FastText_embeddings.txt to FastText_embeddings.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQnDnPBRfobF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "378d4738-4c9d-41b3-9eaa-a49365800058"
      },
      "source": [
        "array = np.loadtxt('FastText_embeddings.txt')\n",
        "array"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.02792864,  0.04547333, -0.02749095, ..., -0.01844831,\n",
              "        -0.07049414,  0.02148656],\n",
              "       [-0.00413048,  0.02610454, -0.04540325, ...,  0.00562713,\n",
              "        -0.07395396,  0.0368084 ],\n",
              "       [-0.01384597,  0.03443255, -0.04860777, ..., -0.00159635,\n",
              "        -0.07496523,  0.04420214],\n",
              "       ...,\n",
              "       [-0.03404125,  0.06368995, -0.0450493 , ..., -0.02192115,\n",
              "        -0.07986488,  0.04349259],\n",
              "       [-0.0221134 ,  0.06743413, -0.03308356, ..., -0.01464292,\n",
              "        -0.06522159,  0.0383691 ],\n",
              "       [-0.03598973,  0.07643144, -0.0546134 , ..., -0.014615  ,\n",
              "        -0.06654139,  0.05352054]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXgmGWx93JTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(df)\n",
        "embeddings = {}\n",
        "for i in range(n):\n",
        "  embeddings[i] = array[i]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrtufBV23P5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "1f204fed-220e-4978-cde4-edbddc7a1fcc"
      },
      "source": [
        "embedding_df = pd.DataFrame.from_dict(embeddings, orient = 'index')\n",
        "embedding_df['label'] = df['label']\n",
        "embedding_df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.027929</td>\n",
              "      <td>0.045473</td>\n",
              "      <td>-0.027491</td>\n",
              "      <td>0.015190</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>-0.024935</td>\n",
              "      <td>-0.032980</td>\n",
              "      <td>0.008431</td>\n",
              "      <td>-0.037732</td>\n",
              "      <td>0.030084</td>\n",
              "      <td>0.016702</td>\n",
              "      <td>0.064239</td>\n",
              "      <td>0.014436</td>\n",
              "      <td>-0.027693</td>\n",
              "      <td>-0.007810</td>\n",
              "      <td>-0.029422</td>\n",
              "      <td>-0.003710</td>\n",
              "      <td>0.021838</td>\n",
              "      <td>0.023579</td>\n",
              "      <td>0.016537</td>\n",
              "      <td>-0.014005</td>\n",
              "      <td>0.022275</td>\n",
              "      <td>-0.053586</td>\n",
              "      <td>-0.025039</td>\n",
              "      <td>0.019068</td>\n",
              "      <td>-0.040851</td>\n",
              "      <td>-0.054432</td>\n",
              "      <td>0.041027</td>\n",
              "      <td>0.003569</td>\n",
              "      <td>-0.010553</td>\n",
              "      <td>-0.023212</td>\n",
              "      <td>-0.005068</td>\n",
              "      <td>0.002998</td>\n",
              "      <td>0.005275</td>\n",
              "      <td>-0.024828</td>\n",
              "      <td>-0.000735</td>\n",
              "      <td>0.052682</td>\n",
              "      <td>0.013410</td>\n",
              "      <td>-0.021471</td>\n",
              "      <td>-0.032271</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036897</td>\n",
              "      <td>-0.046403</td>\n",
              "      <td>-0.028568</td>\n",
              "      <td>0.035028</td>\n",
              "      <td>0.004339</td>\n",
              "      <td>0.047216</td>\n",
              "      <td>-0.045766</td>\n",
              "      <td>0.015662</td>\n",
              "      <td>-0.069638</td>\n",
              "      <td>0.052229</td>\n",
              "      <td>0.006115</td>\n",
              "      <td>0.023211</td>\n",
              "      <td>0.008596</td>\n",
              "      <td>0.060139</td>\n",
              "      <td>-0.008416</td>\n",
              "      <td>-0.005664</td>\n",
              "      <td>-0.009244</td>\n",
              "      <td>-0.047376</td>\n",
              "      <td>0.047727</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>0.046259</td>\n",
              "      <td>0.067883</td>\n",
              "      <td>-0.008340</td>\n",
              "      <td>-0.041430</td>\n",
              "      <td>0.048779</td>\n",
              "      <td>-0.025553</td>\n",
              "      <td>0.047854</td>\n",
              "      <td>0.038176</td>\n",
              "      <td>0.034293</td>\n",
              "      <td>0.061943</td>\n",
              "      <td>0.019458</td>\n",
              "      <td>-0.014631</td>\n",
              "      <td>0.043594</td>\n",
              "      <td>-0.028582</td>\n",
              "      <td>-0.016275</td>\n",
              "      <td>-0.017250</td>\n",
              "      <td>-0.018448</td>\n",
              "      <td>-0.070494</td>\n",
              "      <td>0.021487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.004130</td>\n",
              "      <td>0.026105</td>\n",
              "      <td>-0.045403</td>\n",
              "      <td>-0.003086</td>\n",
              "      <td>0.004016</td>\n",
              "      <td>-0.022892</td>\n",
              "      <td>-0.008053</td>\n",
              "      <td>0.016219</td>\n",
              "      <td>-0.032984</td>\n",
              "      <td>0.034626</td>\n",
              "      <td>0.007679</td>\n",
              "      <td>0.042517</td>\n",
              "      <td>0.023115</td>\n",
              "      <td>-0.024550</td>\n",
              "      <td>-0.011162</td>\n",
              "      <td>-0.031245</td>\n",
              "      <td>-0.015645</td>\n",
              "      <td>0.022321</td>\n",
              "      <td>0.020165</td>\n",
              "      <td>0.021781</td>\n",
              "      <td>0.007559</td>\n",
              "      <td>0.028865</td>\n",
              "      <td>-0.060094</td>\n",
              "      <td>-0.006792</td>\n",
              "      <td>0.028574</td>\n",
              "      <td>-0.047876</td>\n",
              "      <td>-0.052358</td>\n",
              "      <td>0.037748</td>\n",
              "      <td>0.004507</td>\n",
              "      <td>-0.018644</td>\n",
              "      <td>-0.005876</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>-0.007142</td>\n",
              "      <td>-0.011379</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.040178</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>-0.020116</td>\n",
              "      <td>-0.039740</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059650</td>\n",
              "      <td>-0.049295</td>\n",
              "      <td>-0.017975</td>\n",
              "      <td>0.030011</td>\n",
              "      <td>-0.002585</td>\n",
              "      <td>0.020801</td>\n",
              "      <td>-0.046827</td>\n",
              "      <td>0.018261</td>\n",
              "      <td>-0.074041</td>\n",
              "      <td>0.046927</td>\n",
              "      <td>-0.012671</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>-0.009282</td>\n",
              "      <td>0.049995</td>\n",
              "      <td>0.001530</td>\n",
              "      <td>-0.014517</td>\n",
              "      <td>-0.002215</td>\n",
              "      <td>-0.048543</td>\n",
              "      <td>0.034480</td>\n",
              "      <td>-0.025405</td>\n",
              "      <td>0.031593</td>\n",
              "      <td>0.054605</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>-0.048124</td>\n",
              "      <td>0.026710</td>\n",
              "      <td>-0.034582</td>\n",
              "      <td>0.042300</td>\n",
              "      <td>0.050427</td>\n",
              "      <td>0.047222</td>\n",
              "      <td>0.054603</td>\n",
              "      <td>0.032794</td>\n",
              "      <td>-0.015163</td>\n",
              "      <td>0.041301</td>\n",
              "      <td>-0.040048</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>-0.010156</td>\n",
              "      <td>0.005627</td>\n",
              "      <td>-0.073954</td>\n",
              "      <td>0.036808</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.013846</td>\n",
              "      <td>0.034433</td>\n",
              "      <td>-0.048608</td>\n",
              "      <td>-0.007782</td>\n",
              "      <td>-0.002638</td>\n",
              "      <td>-0.020973</td>\n",
              "      <td>-0.014225</td>\n",
              "      <td>0.014021</td>\n",
              "      <td>-0.033846</td>\n",
              "      <td>0.042294</td>\n",
              "      <td>0.012886</td>\n",
              "      <td>0.046607</td>\n",
              "      <td>0.025992</td>\n",
              "      <td>-0.029386</td>\n",
              "      <td>-0.005675</td>\n",
              "      <td>-0.030501</td>\n",
              "      <td>-0.017116</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.024673</td>\n",
              "      <td>0.020325</td>\n",
              "      <td>0.009300</td>\n",
              "      <td>0.024187</td>\n",
              "      <td>-0.063379</td>\n",
              "      <td>-0.010718</td>\n",
              "      <td>0.021502</td>\n",
              "      <td>-0.056413</td>\n",
              "      <td>-0.058291</td>\n",
              "      <td>0.037070</td>\n",
              "      <td>0.004613</td>\n",
              "      <td>-0.014826</td>\n",
              "      <td>-0.015049</td>\n",
              "      <td>-0.001719</td>\n",
              "      <td>-0.000801</td>\n",
              "      <td>-0.002586</td>\n",
              "      <td>-0.017067</td>\n",
              "      <td>-0.004330</td>\n",
              "      <td>0.041612</td>\n",
              "      <td>0.005221</td>\n",
              "      <td>-0.030770</td>\n",
              "      <td>-0.035603</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064650</td>\n",
              "      <td>-0.045617</td>\n",
              "      <td>-0.023339</td>\n",
              "      <td>0.028378</td>\n",
              "      <td>-0.007444</td>\n",
              "      <td>0.022630</td>\n",
              "      <td>-0.050858</td>\n",
              "      <td>0.017568</td>\n",
              "      <td>-0.064044</td>\n",
              "      <td>0.047588</td>\n",
              "      <td>-0.021169</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>-0.002921</td>\n",
              "      <td>0.067823</td>\n",
              "      <td>-0.000294</td>\n",
              "      <td>-0.013041</td>\n",
              "      <td>0.004509</td>\n",
              "      <td>-0.053449</td>\n",
              "      <td>0.036137</td>\n",
              "      <td>-0.020958</td>\n",
              "      <td>0.023003</td>\n",
              "      <td>0.057330</td>\n",
              "      <td>0.005377</td>\n",
              "      <td>-0.049994</td>\n",
              "      <td>0.028060</td>\n",
              "      <td>-0.040692</td>\n",
              "      <td>0.039361</td>\n",
              "      <td>0.047700</td>\n",
              "      <td>0.048854</td>\n",
              "      <td>0.058045</td>\n",
              "      <td>0.030292</td>\n",
              "      <td>-0.010244</td>\n",
              "      <td>0.043663</td>\n",
              "      <td>-0.041667</td>\n",
              "      <td>-0.007432</td>\n",
              "      <td>-0.012866</td>\n",
              "      <td>-0.001596</td>\n",
              "      <td>-0.074965</td>\n",
              "      <td>0.044202</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.020380</td>\n",
              "      <td>0.062005</td>\n",
              "      <td>-0.036003</td>\n",
              "      <td>0.003629</td>\n",
              "      <td>-0.004798</td>\n",
              "      <td>-0.028179</td>\n",
              "      <td>-0.043848</td>\n",
              "      <td>0.003861</td>\n",
              "      <td>-0.049418</td>\n",
              "      <td>0.037786</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.062605</td>\n",
              "      <td>0.017976</td>\n",
              "      <td>-0.018270</td>\n",
              "      <td>0.001802</td>\n",
              "      <td>-0.029008</td>\n",
              "      <td>0.004497</td>\n",
              "      <td>0.037384</td>\n",
              "      <td>0.032797</td>\n",
              "      <td>0.013666</td>\n",
              "      <td>-0.013678</td>\n",
              "      <td>0.005107</td>\n",
              "      <td>-0.052585</td>\n",
              "      <td>-0.032091</td>\n",
              "      <td>0.023455</td>\n",
              "      <td>-0.039506</td>\n",
              "      <td>-0.071952</td>\n",
              "      <td>0.041092</td>\n",
              "      <td>0.008768</td>\n",
              "      <td>-0.012441</td>\n",
              "      <td>-0.021543</td>\n",
              "      <td>-0.013440</td>\n",
              "      <td>-0.011511</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>-0.016868</td>\n",
              "      <td>-0.003797</td>\n",
              "      <td>0.062168</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>-0.017684</td>\n",
              "      <td>-0.033057</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036466</td>\n",
              "      <td>-0.048071</td>\n",
              "      <td>-0.044052</td>\n",
              "      <td>0.031582</td>\n",
              "      <td>-0.005607</td>\n",
              "      <td>0.046977</td>\n",
              "      <td>-0.046315</td>\n",
              "      <td>0.012168</td>\n",
              "      <td>-0.049222</td>\n",
              "      <td>0.055603</td>\n",
              "      <td>-0.008354</td>\n",
              "      <td>0.027261</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.057126</td>\n",
              "      <td>0.008092</td>\n",
              "      <td>-0.002842</td>\n",
              "      <td>-0.010246</td>\n",
              "      <td>-0.037934</td>\n",
              "      <td>0.049191</td>\n",
              "      <td>-0.018213</td>\n",
              "      <td>0.045192</td>\n",
              "      <td>0.095404</td>\n",
              "      <td>-0.007928</td>\n",
              "      <td>-0.039156</td>\n",
              "      <td>0.048771</td>\n",
              "      <td>-0.034426</td>\n",
              "      <td>0.038492</td>\n",
              "      <td>0.039559</td>\n",
              "      <td>0.031308</td>\n",
              "      <td>0.052141</td>\n",
              "      <td>0.027275</td>\n",
              "      <td>-0.008246</td>\n",
              "      <td>0.043803</td>\n",
              "      <td>-0.032329</td>\n",
              "      <td>-0.021422</td>\n",
              "      <td>-0.018111</td>\n",
              "      <td>-0.009990</td>\n",
              "      <td>-0.060476</td>\n",
              "      <td>0.027701</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.003644</td>\n",
              "      <td>0.028737</td>\n",
              "      <td>-0.052126</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>0.008107</td>\n",
              "      <td>-0.017344</td>\n",
              "      <td>-0.013791</td>\n",
              "      <td>0.012748</td>\n",
              "      <td>-0.035662</td>\n",
              "      <td>0.039845</td>\n",
              "      <td>0.006583</td>\n",
              "      <td>0.042427</td>\n",
              "      <td>0.022603</td>\n",
              "      <td>-0.026439</td>\n",
              "      <td>-0.011025</td>\n",
              "      <td>-0.032833</td>\n",
              "      <td>-0.010979</td>\n",
              "      <td>0.014916</td>\n",
              "      <td>0.026037</td>\n",
              "      <td>0.021582</td>\n",
              "      <td>0.004839</td>\n",
              "      <td>0.028031</td>\n",
              "      <td>-0.069568</td>\n",
              "      <td>-0.010970</td>\n",
              "      <td>0.035693</td>\n",
              "      <td>-0.046544</td>\n",
              "      <td>-0.054415</td>\n",
              "      <td>0.052412</td>\n",
              "      <td>0.012303</td>\n",
              "      <td>-0.015019</td>\n",
              "      <td>-0.008838</td>\n",
              "      <td>-0.008885</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>-0.009247</td>\n",
              "      <td>-0.010716</td>\n",
              "      <td>-0.001443</td>\n",
              "      <td>0.034402</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>-0.020472</td>\n",
              "      <td>-0.038950</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.058288</td>\n",
              "      <td>-0.054549</td>\n",
              "      <td>-0.024948</td>\n",
              "      <td>0.033424</td>\n",
              "      <td>-0.003804</td>\n",
              "      <td>0.013218</td>\n",
              "      <td>-0.054962</td>\n",
              "      <td>0.012327</td>\n",
              "      <td>-0.076053</td>\n",
              "      <td>0.053361</td>\n",
              "      <td>-0.018637</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>-0.005087</td>\n",
              "      <td>0.054736</td>\n",
              "      <td>-0.002094</td>\n",
              "      <td>-0.014366</td>\n",
              "      <td>0.003204</td>\n",
              "      <td>-0.057732</td>\n",
              "      <td>0.036873</td>\n",
              "      <td>-0.031804</td>\n",
              "      <td>0.027221</td>\n",
              "      <td>0.055413</td>\n",
              "      <td>0.007639</td>\n",
              "      <td>-0.042472</td>\n",
              "      <td>0.022434</td>\n",
              "      <td>-0.037340</td>\n",
              "      <td>0.041808</td>\n",
              "      <td>0.061506</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>0.056493</td>\n",
              "      <td>0.034941</td>\n",
              "      <td>-0.020758</td>\n",
              "      <td>0.042536</td>\n",
              "      <td>-0.039278</td>\n",
              "      <td>-0.004636</td>\n",
              "      <td>-0.008690</td>\n",
              "      <td>0.009474</td>\n",
              "      <td>-0.067990</td>\n",
              "      <td>0.039390</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>-0.014070</td>\n",
              "      <td>0.051215</td>\n",
              "      <td>-0.035568</td>\n",
              "      <td>0.012257</td>\n",
              "      <td>0.005194</td>\n",
              "      <td>-0.015653</td>\n",
              "      <td>-0.042579</td>\n",
              "      <td>0.012735</td>\n",
              "      <td>-0.052595</td>\n",
              "      <td>0.034248</td>\n",
              "      <td>0.000557</td>\n",
              "      <td>0.062564</td>\n",
              "      <td>0.016335</td>\n",
              "      <td>-0.013081</td>\n",
              "      <td>-0.010573</td>\n",
              "      <td>-0.033486</td>\n",
              "      <td>-0.001452</td>\n",
              "      <td>0.022507</td>\n",
              "      <td>0.033711</td>\n",
              "      <td>0.033569</td>\n",
              "      <td>-0.021113</td>\n",
              "      <td>0.029060</td>\n",
              "      <td>-0.052718</td>\n",
              "      <td>-0.033622</td>\n",
              "      <td>0.023056</td>\n",
              "      <td>-0.044935</td>\n",
              "      <td>-0.060308</td>\n",
              "      <td>0.051118</td>\n",
              "      <td>0.006564</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>-0.005219</td>\n",
              "      <td>-0.006479</td>\n",
              "      <td>0.006326</td>\n",
              "      <td>0.012828</td>\n",
              "      <td>-0.016821</td>\n",
              "      <td>-0.002109</td>\n",
              "      <td>0.041647</td>\n",
              "      <td>0.017541</td>\n",
              "      <td>-0.021698</td>\n",
              "      <td>-0.040867</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.032618</td>\n",
              "      <td>-0.051756</td>\n",
              "      <td>-0.038423</td>\n",
              "      <td>0.035258</td>\n",
              "      <td>0.004712</td>\n",
              "      <td>0.030754</td>\n",
              "      <td>-0.054664</td>\n",
              "      <td>0.004134</td>\n",
              "      <td>-0.060256</td>\n",
              "      <td>0.056926</td>\n",
              "      <td>-0.009551</td>\n",
              "      <td>0.016278</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.048969</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.002043</td>\n",
              "      <td>0.005452</td>\n",
              "      <td>-0.047875</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>-0.026212</td>\n",
              "      <td>0.035533</td>\n",
              "      <td>0.060709</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>-0.029753</td>\n",
              "      <td>0.031572</td>\n",
              "      <td>-0.028518</td>\n",
              "      <td>0.043532</td>\n",
              "      <td>0.033738</td>\n",
              "      <td>0.042854</td>\n",
              "      <td>0.052159</td>\n",
              "      <td>0.022012</td>\n",
              "      <td>-0.020773</td>\n",
              "      <td>0.036234</td>\n",
              "      <td>-0.029974</td>\n",
              "      <td>-0.002367</td>\n",
              "      <td>-0.006040</td>\n",
              "      <td>-0.014287</td>\n",
              "      <td>-0.057650</td>\n",
              "      <td>0.031595</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2565</th>\n",
              "      <td>-0.010778</td>\n",
              "      <td>0.052305</td>\n",
              "      <td>-0.028607</td>\n",
              "      <td>-0.004237</td>\n",
              "      <td>0.010796</td>\n",
              "      <td>-0.017589</td>\n",
              "      <td>-0.048365</td>\n",
              "      <td>0.011366</td>\n",
              "      <td>-0.053011</td>\n",
              "      <td>0.036167</td>\n",
              "      <td>-0.007927</td>\n",
              "      <td>0.052547</td>\n",
              "      <td>0.018993</td>\n",
              "      <td>-0.007495</td>\n",
              "      <td>-0.006449</td>\n",
              "      <td>-0.026048</td>\n",
              "      <td>-0.013240</td>\n",
              "      <td>0.033213</td>\n",
              "      <td>0.033010</td>\n",
              "      <td>0.034916</td>\n",
              "      <td>-0.012613</td>\n",
              "      <td>0.031278</td>\n",
              "      <td>-0.045218</td>\n",
              "      <td>-0.036640</td>\n",
              "      <td>0.024128</td>\n",
              "      <td>-0.061104</td>\n",
              "      <td>-0.069656</td>\n",
              "      <td>0.050010</td>\n",
              "      <td>0.005394</td>\n",
              "      <td>-0.005350</td>\n",
              "      <td>0.005893</td>\n",
              "      <td>0.010049</td>\n",
              "      <td>0.003262</td>\n",
              "      <td>0.020359</td>\n",
              "      <td>-0.014422</td>\n",
              "      <td>0.003393</td>\n",
              "      <td>0.041758</td>\n",
              "      <td>0.008783</td>\n",
              "      <td>-0.031495</td>\n",
              "      <td>-0.039425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040726</td>\n",
              "      <td>-0.045586</td>\n",
              "      <td>-0.033005</td>\n",
              "      <td>0.031834</td>\n",
              "      <td>-0.000447</td>\n",
              "      <td>0.018225</td>\n",
              "      <td>-0.054425</td>\n",
              "      <td>0.004326</td>\n",
              "      <td>-0.066990</td>\n",
              "      <td>0.056175</td>\n",
              "      <td>-0.023542</td>\n",
              "      <td>0.010346</td>\n",
              "      <td>-0.003475</td>\n",
              "      <td>0.059207</td>\n",
              "      <td>0.007164</td>\n",
              "      <td>-0.004567</td>\n",
              "      <td>0.011624</td>\n",
              "      <td>-0.056219</td>\n",
              "      <td>0.044492</td>\n",
              "      <td>-0.032875</td>\n",
              "      <td>0.039719</td>\n",
              "      <td>0.053722</td>\n",
              "      <td>0.007868</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.014236</td>\n",
              "      <td>-0.025566</td>\n",
              "      <td>0.034003</td>\n",
              "      <td>0.035165</td>\n",
              "      <td>0.042424</td>\n",
              "      <td>0.054775</td>\n",
              "      <td>0.017937</td>\n",
              "      <td>-0.032164</td>\n",
              "      <td>0.042074</td>\n",
              "      <td>-0.044059</td>\n",
              "      <td>-0.002813</td>\n",
              "      <td>-0.001419</td>\n",
              "      <td>-0.005981</td>\n",
              "      <td>-0.066244</td>\n",
              "      <td>0.036849</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2566</th>\n",
              "      <td>-0.034041</td>\n",
              "      <td>0.063690</td>\n",
              "      <td>-0.045049</td>\n",
              "      <td>0.011171</td>\n",
              "      <td>0.008439</td>\n",
              "      <td>-0.018975</td>\n",
              "      <td>-0.064149</td>\n",
              "      <td>0.010974</td>\n",
              "      <td>-0.042033</td>\n",
              "      <td>0.037381</td>\n",
              "      <td>0.013586</td>\n",
              "      <td>0.068683</td>\n",
              "      <td>0.020153</td>\n",
              "      <td>-0.029022</td>\n",
              "      <td>-0.022758</td>\n",
              "      <td>-0.028592</td>\n",
              "      <td>-0.016705</td>\n",
              "      <td>0.022119</td>\n",
              "      <td>0.033945</td>\n",
              "      <td>0.025475</td>\n",
              "      <td>-0.015793</td>\n",
              "      <td>0.044104</td>\n",
              "      <td>-0.051808</td>\n",
              "      <td>-0.022477</td>\n",
              "      <td>0.013657</td>\n",
              "      <td>-0.050048</td>\n",
              "      <td>-0.057262</td>\n",
              "      <td>0.049558</td>\n",
              "      <td>0.025324</td>\n",
              "      <td>-0.009744</td>\n",
              "      <td>-0.023486</td>\n",
              "      <td>0.008353</td>\n",
              "      <td>0.011844</td>\n",
              "      <td>0.017109</td>\n",
              "      <td>-0.028136</td>\n",
              "      <td>0.022315</td>\n",
              "      <td>0.060012</td>\n",
              "      <td>0.030145</td>\n",
              "      <td>-0.031085</td>\n",
              "      <td>-0.036175</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.043740</td>\n",
              "      <td>-0.041445</td>\n",
              "      <td>-0.030417</td>\n",
              "      <td>0.049290</td>\n",
              "      <td>-0.009159</td>\n",
              "      <td>0.036361</td>\n",
              "      <td>-0.070505</td>\n",
              "      <td>0.005134</td>\n",
              "      <td>-0.067559</td>\n",
              "      <td>0.066006</td>\n",
              "      <td>-0.016549</td>\n",
              "      <td>0.005306</td>\n",
              "      <td>0.004894</td>\n",
              "      <td>0.070250</td>\n",
              "      <td>-0.007686</td>\n",
              "      <td>0.007941</td>\n",
              "      <td>0.006906</td>\n",
              "      <td>-0.069525</td>\n",
              "      <td>0.049911</td>\n",
              "      <td>-0.008691</td>\n",
              "      <td>0.042387</td>\n",
              "      <td>0.054764</td>\n",
              "      <td>-0.006719</td>\n",
              "      <td>-0.040958</td>\n",
              "      <td>0.045897</td>\n",
              "      <td>-0.024717</td>\n",
              "      <td>0.066032</td>\n",
              "      <td>0.045838</td>\n",
              "      <td>0.055704</td>\n",
              "      <td>0.062179</td>\n",
              "      <td>0.004410</td>\n",
              "      <td>-0.039957</td>\n",
              "      <td>0.042697</td>\n",
              "      <td>-0.044733</td>\n",
              "      <td>-0.026958</td>\n",
              "      <td>0.005858</td>\n",
              "      <td>-0.021921</td>\n",
              "      <td>-0.079865</td>\n",
              "      <td>0.043493</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2567</th>\n",
              "      <td>-0.022113</td>\n",
              "      <td>0.067434</td>\n",
              "      <td>-0.033084</td>\n",
              "      <td>0.010749</td>\n",
              "      <td>0.011717</td>\n",
              "      <td>-0.017393</td>\n",
              "      <td>-0.060626</td>\n",
              "      <td>0.007116</td>\n",
              "      <td>-0.050615</td>\n",
              "      <td>0.037305</td>\n",
              "      <td>-0.000198</td>\n",
              "      <td>0.065772</td>\n",
              "      <td>0.021000</td>\n",
              "      <td>-0.019481</td>\n",
              "      <td>-0.010695</td>\n",
              "      <td>-0.030553</td>\n",
              "      <td>-0.020217</td>\n",
              "      <td>0.027489</td>\n",
              "      <td>0.040159</td>\n",
              "      <td>0.030758</td>\n",
              "      <td>-0.019741</td>\n",
              "      <td>0.035046</td>\n",
              "      <td>-0.049159</td>\n",
              "      <td>-0.025179</td>\n",
              "      <td>0.017275</td>\n",
              "      <td>-0.050540</td>\n",
              "      <td>-0.068399</td>\n",
              "      <td>0.063924</td>\n",
              "      <td>0.024356</td>\n",
              "      <td>-0.009533</td>\n",
              "      <td>-0.013464</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>0.004121</td>\n",
              "      <td>0.018984</td>\n",
              "      <td>-0.018626</td>\n",
              "      <td>0.010138</td>\n",
              "      <td>0.051358</td>\n",
              "      <td>0.025818</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>-0.040970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042648</td>\n",
              "      <td>-0.040011</td>\n",
              "      <td>-0.029103</td>\n",
              "      <td>0.049968</td>\n",
              "      <td>-0.006738</td>\n",
              "      <td>0.033006</td>\n",
              "      <td>-0.061539</td>\n",
              "      <td>0.006566</td>\n",
              "      <td>-0.065520</td>\n",
              "      <td>0.068365</td>\n",
              "      <td>-0.028171</td>\n",
              "      <td>0.008543</td>\n",
              "      <td>0.009916</td>\n",
              "      <td>0.067359</td>\n",
              "      <td>-0.002604</td>\n",
              "      <td>-0.001650</td>\n",
              "      <td>0.016771</td>\n",
              "      <td>-0.064565</td>\n",
              "      <td>0.048446</td>\n",
              "      <td>-0.023304</td>\n",
              "      <td>0.046396</td>\n",
              "      <td>0.055635</td>\n",
              "      <td>-0.008334</td>\n",
              "      <td>-0.036695</td>\n",
              "      <td>0.027267</td>\n",
              "      <td>-0.027365</td>\n",
              "      <td>0.052087</td>\n",
              "      <td>0.036607</td>\n",
              "      <td>0.049080</td>\n",
              "      <td>0.055231</td>\n",
              "      <td>0.005515</td>\n",
              "      <td>-0.039320</td>\n",
              "      <td>0.042067</td>\n",
              "      <td>-0.040079</td>\n",
              "      <td>-0.023739</td>\n",
              "      <td>0.002317</td>\n",
              "      <td>-0.014643</td>\n",
              "      <td>-0.065222</td>\n",
              "      <td>0.038369</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2568</th>\n",
              "      <td>-0.035990</td>\n",
              "      <td>0.076431</td>\n",
              "      <td>-0.054613</td>\n",
              "      <td>0.018410</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>-0.010889</td>\n",
              "      <td>-0.066221</td>\n",
              "      <td>0.011762</td>\n",
              "      <td>-0.059287</td>\n",
              "      <td>0.042753</td>\n",
              "      <td>0.003151</td>\n",
              "      <td>0.076372</td>\n",
              "      <td>0.021354</td>\n",
              "      <td>-0.013342</td>\n",
              "      <td>-0.015227</td>\n",
              "      <td>-0.019678</td>\n",
              "      <td>-0.000638</td>\n",
              "      <td>0.031540</td>\n",
              "      <td>0.044715</td>\n",
              "      <td>0.036452</td>\n",
              "      <td>-0.019440</td>\n",
              "      <td>0.041967</td>\n",
              "      <td>-0.064899</td>\n",
              "      <td>-0.032887</td>\n",
              "      <td>0.016772</td>\n",
              "      <td>-0.050065</td>\n",
              "      <td>-0.067165</td>\n",
              "      <td>0.058931</td>\n",
              "      <td>0.025334</td>\n",
              "      <td>-0.011251</td>\n",
              "      <td>-0.016483</td>\n",
              "      <td>0.005107</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>0.020560</td>\n",
              "      <td>-0.012529</td>\n",
              "      <td>0.010646</td>\n",
              "      <td>0.053850</td>\n",
              "      <td>0.038558</td>\n",
              "      <td>-0.036735</td>\n",
              "      <td>-0.038626</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051133</td>\n",
              "      <td>-0.045149</td>\n",
              "      <td>-0.039694</td>\n",
              "      <td>0.051260</td>\n",
              "      <td>-0.003349</td>\n",
              "      <td>0.046764</td>\n",
              "      <td>-0.074114</td>\n",
              "      <td>-0.001239</td>\n",
              "      <td>-0.061482</td>\n",
              "      <td>0.073151</td>\n",
              "      <td>-0.012539</td>\n",
              "      <td>0.012179</td>\n",
              "      <td>0.007360</td>\n",
              "      <td>0.073511</td>\n",
              "      <td>-0.007186</td>\n",
              "      <td>0.001609</td>\n",
              "      <td>0.014455</td>\n",
              "      <td>-0.063706</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>-0.012403</td>\n",
              "      <td>0.040878</td>\n",
              "      <td>0.079306</td>\n",
              "      <td>-0.010657</td>\n",
              "      <td>-0.033383</td>\n",
              "      <td>0.047993</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>0.062628</td>\n",
              "      <td>0.037927</td>\n",
              "      <td>0.063824</td>\n",
              "      <td>0.063853</td>\n",
              "      <td>0.015976</td>\n",
              "      <td>-0.030418</td>\n",
              "      <td>0.044989</td>\n",
              "      <td>-0.050974</td>\n",
              "      <td>-0.027303</td>\n",
              "      <td>0.004480</td>\n",
              "      <td>-0.014615</td>\n",
              "      <td>-0.066541</td>\n",
              "      <td>0.053521</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2569 rows  301 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2  ...       298       299  label\n",
              "0    -0.027929  0.045473 -0.027491  ... -0.070494  0.021487      1\n",
              "1    -0.004130  0.026105 -0.045403  ... -0.073954  0.036808      0\n",
              "2    -0.013846  0.034433 -0.048608  ... -0.074965  0.044202      1\n",
              "3    -0.020380  0.062005 -0.036003  ... -0.060476  0.027701      1\n",
              "4    -0.003644  0.028737 -0.052126  ... -0.067990  0.039390      0\n",
              "...        ...       ...       ...  ...       ...       ...    ...\n",
              "2564 -0.014070  0.051215 -0.035568  ... -0.057650  0.031595      0\n",
              "2565 -0.010778  0.052305 -0.028607  ... -0.066244  0.036849      0\n",
              "2566 -0.034041  0.063690 -0.045049  ... -0.079865  0.043493      0\n",
              "2567 -0.022113  0.067434 -0.033084  ... -0.065222  0.038369      0\n",
              "2568 -0.035990  0.076431 -0.054613  ... -0.066541  0.053521      0\n",
              "\n",
              "[2569 rows x 301 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6XEft713erD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "a657ca61-4f12-4b84-a745-1fe3d93e3f3c"
      },
      "source": [
        "X = embedding_df.drop(['label'], axis=1)\n",
        "X.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.027929</td>\n",
              "      <td>0.045473</td>\n",
              "      <td>-0.027491</td>\n",
              "      <td>0.015190</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>-0.024935</td>\n",
              "      <td>-0.032980</td>\n",
              "      <td>0.008431</td>\n",
              "      <td>-0.037732</td>\n",
              "      <td>0.030084</td>\n",
              "      <td>0.016702</td>\n",
              "      <td>0.064239</td>\n",
              "      <td>0.014436</td>\n",
              "      <td>-0.027693</td>\n",
              "      <td>-0.007810</td>\n",
              "      <td>-0.029422</td>\n",
              "      <td>-0.003710</td>\n",
              "      <td>0.021838</td>\n",
              "      <td>0.023579</td>\n",
              "      <td>0.016537</td>\n",
              "      <td>-0.014005</td>\n",
              "      <td>0.022275</td>\n",
              "      <td>-0.053586</td>\n",
              "      <td>-0.025039</td>\n",
              "      <td>0.019068</td>\n",
              "      <td>-0.040851</td>\n",
              "      <td>-0.054432</td>\n",
              "      <td>0.041027</td>\n",
              "      <td>0.003569</td>\n",
              "      <td>-0.010553</td>\n",
              "      <td>-0.023212</td>\n",
              "      <td>-0.005068</td>\n",
              "      <td>0.002998</td>\n",
              "      <td>0.005275</td>\n",
              "      <td>-0.024828</td>\n",
              "      <td>-0.000735</td>\n",
              "      <td>0.052682</td>\n",
              "      <td>0.013410</td>\n",
              "      <td>-0.021471</td>\n",
              "      <td>-0.032271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051648</td>\n",
              "      <td>-0.036897</td>\n",
              "      <td>-0.046403</td>\n",
              "      <td>-0.028568</td>\n",
              "      <td>0.035028</td>\n",
              "      <td>0.004339</td>\n",
              "      <td>0.047216</td>\n",
              "      <td>-0.045766</td>\n",
              "      <td>0.015662</td>\n",
              "      <td>-0.069638</td>\n",
              "      <td>0.052229</td>\n",
              "      <td>0.006115</td>\n",
              "      <td>0.023211</td>\n",
              "      <td>0.008596</td>\n",
              "      <td>0.060139</td>\n",
              "      <td>-0.008416</td>\n",
              "      <td>-0.005664</td>\n",
              "      <td>-0.009244</td>\n",
              "      <td>-0.047376</td>\n",
              "      <td>0.047727</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>0.046259</td>\n",
              "      <td>0.067883</td>\n",
              "      <td>-0.008340</td>\n",
              "      <td>-0.041430</td>\n",
              "      <td>0.048779</td>\n",
              "      <td>-0.025553</td>\n",
              "      <td>0.047854</td>\n",
              "      <td>0.038176</td>\n",
              "      <td>0.034293</td>\n",
              "      <td>0.061943</td>\n",
              "      <td>0.019458</td>\n",
              "      <td>-0.014631</td>\n",
              "      <td>0.043594</td>\n",
              "      <td>-0.028582</td>\n",
              "      <td>-0.016275</td>\n",
              "      <td>-0.017250</td>\n",
              "      <td>-0.018448</td>\n",
              "      <td>-0.070494</td>\n",
              "      <td>0.021487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.004130</td>\n",
              "      <td>0.026105</td>\n",
              "      <td>-0.045403</td>\n",
              "      <td>-0.003086</td>\n",
              "      <td>0.004016</td>\n",
              "      <td>-0.022892</td>\n",
              "      <td>-0.008053</td>\n",
              "      <td>0.016219</td>\n",
              "      <td>-0.032984</td>\n",
              "      <td>0.034626</td>\n",
              "      <td>0.007679</td>\n",
              "      <td>0.042517</td>\n",
              "      <td>0.023115</td>\n",
              "      <td>-0.024550</td>\n",
              "      <td>-0.011162</td>\n",
              "      <td>-0.031245</td>\n",
              "      <td>-0.015645</td>\n",
              "      <td>0.022321</td>\n",
              "      <td>0.020165</td>\n",
              "      <td>0.021781</td>\n",
              "      <td>0.007559</td>\n",
              "      <td>0.028865</td>\n",
              "      <td>-0.060094</td>\n",
              "      <td>-0.006792</td>\n",
              "      <td>0.028574</td>\n",
              "      <td>-0.047876</td>\n",
              "      <td>-0.052358</td>\n",
              "      <td>0.037748</td>\n",
              "      <td>0.004507</td>\n",
              "      <td>-0.018644</td>\n",
              "      <td>-0.005876</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>-0.007142</td>\n",
              "      <td>-0.011379</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.040178</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>-0.020116</td>\n",
              "      <td>-0.039740</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051488</td>\n",
              "      <td>-0.059650</td>\n",
              "      <td>-0.049295</td>\n",
              "      <td>-0.017975</td>\n",
              "      <td>0.030011</td>\n",
              "      <td>-0.002585</td>\n",
              "      <td>0.020801</td>\n",
              "      <td>-0.046827</td>\n",
              "      <td>0.018261</td>\n",
              "      <td>-0.074041</td>\n",
              "      <td>0.046927</td>\n",
              "      <td>-0.012671</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>-0.009282</td>\n",
              "      <td>0.049995</td>\n",
              "      <td>0.001530</td>\n",
              "      <td>-0.014517</td>\n",
              "      <td>-0.002215</td>\n",
              "      <td>-0.048543</td>\n",
              "      <td>0.034480</td>\n",
              "      <td>-0.025405</td>\n",
              "      <td>0.031593</td>\n",
              "      <td>0.054605</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>-0.048124</td>\n",
              "      <td>0.026710</td>\n",
              "      <td>-0.034582</td>\n",
              "      <td>0.042300</td>\n",
              "      <td>0.050427</td>\n",
              "      <td>0.047222</td>\n",
              "      <td>0.054603</td>\n",
              "      <td>0.032794</td>\n",
              "      <td>-0.015163</td>\n",
              "      <td>0.041301</td>\n",
              "      <td>-0.040048</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>-0.010156</td>\n",
              "      <td>0.005627</td>\n",
              "      <td>-0.073954</td>\n",
              "      <td>0.036808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.013846</td>\n",
              "      <td>0.034433</td>\n",
              "      <td>-0.048608</td>\n",
              "      <td>-0.007782</td>\n",
              "      <td>-0.002638</td>\n",
              "      <td>-0.020973</td>\n",
              "      <td>-0.014225</td>\n",
              "      <td>0.014021</td>\n",
              "      <td>-0.033846</td>\n",
              "      <td>0.042294</td>\n",
              "      <td>0.012886</td>\n",
              "      <td>0.046607</td>\n",
              "      <td>0.025992</td>\n",
              "      <td>-0.029386</td>\n",
              "      <td>-0.005675</td>\n",
              "      <td>-0.030501</td>\n",
              "      <td>-0.017116</td>\n",
              "      <td>0.022658</td>\n",
              "      <td>0.024673</td>\n",
              "      <td>0.020325</td>\n",
              "      <td>0.009300</td>\n",
              "      <td>0.024187</td>\n",
              "      <td>-0.063379</td>\n",
              "      <td>-0.010718</td>\n",
              "      <td>0.021502</td>\n",
              "      <td>-0.056413</td>\n",
              "      <td>-0.058291</td>\n",
              "      <td>0.037070</td>\n",
              "      <td>0.004613</td>\n",
              "      <td>-0.014826</td>\n",
              "      <td>-0.015049</td>\n",
              "      <td>-0.001719</td>\n",
              "      <td>-0.000801</td>\n",
              "      <td>-0.002586</td>\n",
              "      <td>-0.017067</td>\n",
              "      <td>-0.004330</td>\n",
              "      <td>0.041612</td>\n",
              "      <td>0.005221</td>\n",
              "      <td>-0.030770</td>\n",
              "      <td>-0.035603</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050406</td>\n",
              "      <td>-0.064650</td>\n",
              "      <td>-0.045617</td>\n",
              "      <td>-0.023339</td>\n",
              "      <td>0.028378</td>\n",
              "      <td>-0.007444</td>\n",
              "      <td>0.022630</td>\n",
              "      <td>-0.050858</td>\n",
              "      <td>0.017568</td>\n",
              "      <td>-0.064044</td>\n",
              "      <td>0.047588</td>\n",
              "      <td>-0.021169</td>\n",
              "      <td>0.002682</td>\n",
              "      <td>-0.002921</td>\n",
              "      <td>0.067823</td>\n",
              "      <td>-0.000294</td>\n",
              "      <td>-0.013041</td>\n",
              "      <td>0.004509</td>\n",
              "      <td>-0.053449</td>\n",
              "      <td>0.036137</td>\n",
              "      <td>-0.020958</td>\n",
              "      <td>0.023003</td>\n",
              "      <td>0.057330</td>\n",
              "      <td>0.005377</td>\n",
              "      <td>-0.049994</td>\n",
              "      <td>0.028060</td>\n",
              "      <td>-0.040692</td>\n",
              "      <td>0.039361</td>\n",
              "      <td>0.047700</td>\n",
              "      <td>0.048854</td>\n",
              "      <td>0.058045</td>\n",
              "      <td>0.030292</td>\n",
              "      <td>-0.010244</td>\n",
              "      <td>0.043663</td>\n",
              "      <td>-0.041667</td>\n",
              "      <td>-0.007432</td>\n",
              "      <td>-0.012866</td>\n",
              "      <td>-0.001596</td>\n",
              "      <td>-0.074965</td>\n",
              "      <td>0.044202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.020380</td>\n",
              "      <td>0.062005</td>\n",
              "      <td>-0.036003</td>\n",
              "      <td>0.003629</td>\n",
              "      <td>-0.004798</td>\n",
              "      <td>-0.028179</td>\n",
              "      <td>-0.043848</td>\n",
              "      <td>0.003861</td>\n",
              "      <td>-0.049418</td>\n",
              "      <td>0.037786</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.062605</td>\n",
              "      <td>0.017976</td>\n",
              "      <td>-0.018270</td>\n",
              "      <td>0.001802</td>\n",
              "      <td>-0.029008</td>\n",
              "      <td>0.004497</td>\n",
              "      <td>0.037384</td>\n",
              "      <td>0.032797</td>\n",
              "      <td>0.013666</td>\n",
              "      <td>-0.013678</td>\n",
              "      <td>0.005107</td>\n",
              "      <td>-0.052585</td>\n",
              "      <td>-0.032091</td>\n",
              "      <td>0.023455</td>\n",
              "      <td>-0.039506</td>\n",
              "      <td>-0.071952</td>\n",
              "      <td>0.041092</td>\n",
              "      <td>0.008768</td>\n",
              "      <td>-0.012441</td>\n",
              "      <td>-0.021543</td>\n",
              "      <td>-0.013440</td>\n",
              "      <td>-0.011511</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>-0.016868</td>\n",
              "      <td>-0.003797</td>\n",
              "      <td>0.062168</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>-0.017684</td>\n",
              "      <td>-0.033057</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052088</td>\n",
              "      <td>-0.036466</td>\n",
              "      <td>-0.048071</td>\n",
              "      <td>-0.044052</td>\n",
              "      <td>0.031582</td>\n",
              "      <td>-0.005607</td>\n",
              "      <td>0.046977</td>\n",
              "      <td>-0.046315</td>\n",
              "      <td>0.012168</td>\n",
              "      <td>-0.049222</td>\n",
              "      <td>0.055603</td>\n",
              "      <td>-0.008354</td>\n",
              "      <td>0.027261</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.057126</td>\n",
              "      <td>0.008092</td>\n",
              "      <td>-0.002842</td>\n",
              "      <td>-0.010246</td>\n",
              "      <td>-0.037934</td>\n",
              "      <td>0.049191</td>\n",
              "      <td>-0.018213</td>\n",
              "      <td>0.045192</td>\n",
              "      <td>0.095404</td>\n",
              "      <td>-0.007928</td>\n",
              "      <td>-0.039156</td>\n",
              "      <td>0.048771</td>\n",
              "      <td>-0.034426</td>\n",
              "      <td>0.038492</td>\n",
              "      <td>0.039559</td>\n",
              "      <td>0.031308</td>\n",
              "      <td>0.052141</td>\n",
              "      <td>0.027275</td>\n",
              "      <td>-0.008246</td>\n",
              "      <td>0.043803</td>\n",
              "      <td>-0.032329</td>\n",
              "      <td>-0.021422</td>\n",
              "      <td>-0.018111</td>\n",
              "      <td>-0.009990</td>\n",
              "      <td>-0.060476</td>\n",
              "      <td>0.027701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.003644</td>\n",
              "      <td>0.028737</td>\n",
              "      <td>-0.052126</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>0.008107</td>\n",
              "      <td>-0.017344</td>\n",
              "      <td>-0.013791</td>\n",
              "      <td>0.012748</td>\n",
              "      <td>-0.035662</td>\n",
              "      <td>0.039845</td>\n",
              "      <td>0.006583</td>\n",
              "      <td>0.042427</td>\n",
              "      <td>0.022603</td>\n",
              "      <td>-0.026439</td>\n",
              "      <td>-0.011025</td>\n",
              "      <td>-0.032833</td>\n",
              "      <td>-0.010979</td>\n",
              "      <td>0.014916</td>\n",
              "      <td>0.026037</td>\n",
              "      <td>0.021582</td>\n",
              "      <td>0.004839</td>\n",
              "      <td>0.028031</td>\n",
              "      <td>-0.069568</td>\n",
              "      <td>-0.010970</td>\n",
              "      <td>0.035693</td>\n",
              "      <td>-0.046544</td>\n",
              "      <td>-0.054415</td>\n",
              "      <td>0.052412</td>\n",
              "      <td>0.012303</td>\n",
              "      <td>-0.015019</td>\n",
              "      <td>-0.008838</td>\n",
              "      <td>-0.008885</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>-0.009247</td>\n",
              "      <td>-0.010716</td>\n",
              "      <td>-0.001443</td>\n",
              "      <td>0.034402</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>-0.020472</td>\n",
              "      <td>-0.038950</td>\n",
              "      <td>...</td>\n",
              "      <td>0.054639</td>\n",
              "      <td>-0.058288</td>\n",
              "      <td>-0.054549</td>\n",
              "      <td>-0.024948</td>\n",
              "      <td>0.033424</td>\n",
              "      <td>-0.003804</td>\n",
              "      <td>0.013218</td>\n",
              "      <td>-0.054962</td>\n",
              "      <td>0.012327</td>\n",
              "      <td>-0.076053</td>\n",
              "      <td>0.053361</td>\n",
              "      <td>-0.018637</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>-0.005087</td>\n",
              "      <td>0.054736</td>\n",
              "      <td>-0.002094</td>\n",
              "      <td>-0.014366</td>\n",
              "      <td>0.003204</td>\n",
              "      <td>-0.057732</td>\n",
              "      <td>0.036873</td>\n",
              "      <td>-0.031804</td>\n",
              "      <td>0.027221</td>\n",
              "      <td>0.055413</td>\n",
              "      <td>0.007639</td>\n",
              "      <td>-0.042472</td>\n",
              "      <td>0.022434</td>\n",
              "      <td>-0.037340</td>\n",
              "      <td>0.041808</td>\n",
              "      <td>0.061506</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>0.056493</td>\n",
              "      <td>0.034941</td>\n",
              "      <td>-0.020758</td>\n",
              "      <td>0.042536</td>\n",
              "      <td>-0.039278</td>\n",
              "      <td>-0.004636</td>\n",
              "      <td>-0.008690</td>\n",
              "      <td>0.009474</td>\n",
              "      <td>-0.067990</td>\n",
              "      <td>0.039390</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       297       298       299\n",
              "0 -0.027929  0.045473 -0.027491  ... -0.018448 -0.070494  0.021487\n",
              "1 -0.004130  0.026105 -0.045403  ...  0.005627 -0.073954  0.036808\n",
              "2 -0.013846  0.034433 -0.048608  ... -0.001596 -0.074965  0.044202\n",
              "3 -0.020380  0.062005 -0.036003  ... -0.009990 -0.060476  0.027701\n",
              "4 -0.003644  0.028737 -0.052126  ...  0.009474 -0.067990  0.039390\n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Q_zxqD3f-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "69f7e079-6dcd-43fe-ec3c-5d8491e1fb57"
      },
      "source": [
        "y = embedding_df['label']\n",
        "y.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    0\n",
              "2    1\n",
              "3    1\n",
              "4    0\n",
              "Name: label, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41_6ewwC3jPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# split into 70:30 ration\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 32)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnIGx_hZ3lhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = [int(x) for x in y_train]\n",
        "y_test = [int(x) for x in y_test]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMumeHvF3qNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlDq1q933tCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an SelectKBest object to select features with two best ANOVA F-Values\n",
        "fvalue_selector = SelectKBest(f_classif, k=300)\n",
        "\n",
        "# Apply the SelectKBest object to the features and target\n",
        "X_kbest_train = fvalue_selector.fit_transform(X_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zwUZOK23vID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an SelectKBest object to select features with two best ANOVA F-Values\n",
        "fvalue_selector = SelectKBest(f_classif, k=300)\n",
        "\n",
        "# Apply the SelectKBest object to the features and target\n",
        "X_kbest_test = fvalue_selector.fit_transform(X_test, y_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V-kE28F-gyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjXtPTOk3x7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = LogisticRegression(random_state=0, C=100, penalty='l2')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4gcYNHD4VIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "a3bde01e-a8b2-480d-8a16-f36f381aff31"
      },
      "source": [
        "#Fitting the model in the training data\n",
        "clf.fit(X_kbest_train, y_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yzEwdxZ4YG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predicting the model\n",
        "y_predict = clf.predict(X_kbest_test)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQVwCqrH4ajH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99e2a141-b354-4844-9f20-5cb47635355c"
      },
      "source": [
        "#accuracy score of the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_predict, normalize=True, sample_weight=None)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.648508430609598"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpi1iUwo4seP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzTv340U4uwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "48cf8ec7-382f-498c-dd6e-fd6055582353"
      },
      "source": [
        "result_svm = confusion_matrix(y_test, y_predict)\n",
        "result_svm"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 16, 312],\n",
              "       [  4, 439]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UszKYHR04weo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47151007-675b-4883-8950-afcaabc93e3f"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV \n",
        "param_grid = {'penalty': ['l1','l2'], \n",
        "              'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, refit = True, verbose = 3)\n",
        "grid.fit(X_kbest_train, y_train)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
            "[CV] C=0.001, penalty=l1 .............................................\n",
            "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.001, penalty=l1 .............................................\n",
            "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.001, penalty=l1 .............................................\n",
            "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.001, penalty=l1 .............................................\n",
            "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.001, penalty=l1 .............................................\n",
            "[CV] ................... C=0.001, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.001, penalty=l2 .............................................\n",
            "[CV] ................. C=0.001, penalty=l2, score=0.597, total=   0.0s\n",
            "[CV] C=0.001, penalty=l2 .............................................\n",
            "[CV] ................. C=0.001, penalty=l2, score=0.594, total=   0.0s\n",
            "[CV] C=0.001, penalty=l2 .............................................\n",
            "[CV] ................. C=0.001, penalty=l2, score=0.594, total=   0.0s\n",
            "[CV] C=0.001, penalty=l2 .............................................\n",
            "[CV] ................. C=0.001, penalty=l2, score=0.596, total=   0.0s\n",
            "[CV] C=0.001, penalty=l2 .............................................\n",
            "[CV] ................. C=0.001, penalty=l2, score=0.596, total=   0.0s\n",
            "[CV] C=0.01, penalty=l1 ..............................................\n",
            "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.01, penalty=l1 ..............................................\n",
            "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.01, penalty=l1 ..............................................\n",
            "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.01, penalty=l1 ..............................................\n",
            "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.01, penalty=l1 ..............................................\n",
            "[CV] .................... C=0.01, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.01, penalty=l2 ..............................................\n",
            "[CV] .................. C=0.01, penalty=l2, score=0.597, total=   0.0s\n",
            "[CV] C=0.01, penalty=l2 ..............................................\n",
            "[CV] .................. C=0.01, penalty=l2, score=0.594, total=   0.0s\n",
            "[CV] C=0.01, penalty=l2 ..............................................\n",
            "[CV] .................. C=0.01, penalty=l2, score=0.594, total=   0.0s\n",
            "[CV] C=0.01, penalty=l2 ..............................................\n",
            "[CV] .................. C=0.01, penalty=l2, score=0.596, total=   0.0s\n",
            "[CV] C=0.01, penalty=l2 ..............................................\n",
            "[CV] .................. C=0.01, penalty=l2, score=0.596, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] C=0.1, penalty=l1 ...............................................\n",
            "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.1, penalty=l1 ...............................................\n",
            "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.1, penalty=l1 ...............................................\n",
            "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.1, penalty=l1 ...............................................\n",
            "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.1, penalty=l1 ...............................................\n",
            "[CV] ..................... C=0.1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=0.1, penalty=l2 ...............................................\n",
            "[CV] ................... C=0.1, penalty=l2, score=0.597, total=   0.0s\n",
            "[CV] C=0.1, penalty=l2 ...............................................\n",
            "[CV] ................... C=0.1, penalty=l2, score=0.594, total=   0.0s\n",
            "[CV] C=0.1, penalty=l2 ...............................................\n",
            "[CV] ................... C=0.1, penalty=l2, score=0.594, total=   0.0s\n",
            "[CV] C=0.1, penalty=l2 ...............................................\n",
            "[CV] ................... C=0.1, penalty=l2, score=0.596, total=   0.0s\n",
            "[CV] C=0.1, penalty=l2 ...............................................\n",
            "[CV] ................... C=0.1, penalty=l2, score=0.596, total=   0.0s\n",
            "[CV] C=1, penalty=l1 .................................................\n",
            "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1, penalty=l1 .................................................\n",
            "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1, penalty=l1 .................................................\n",
            "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1, penalty=l1 .................................................\n",
            "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1, penalty=l1 .................................................\n",
            "[CV] ....................... C=1, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1, penalty=l2 .................................................\n",
            "[CV] ..................... C=1, penalty=l2, score=0.603, total=   0.0s\n",
            "[CV] C=1, penalty=l2 .................................................\n",
            "[CV] ..................... C=1, penalty=l2, score=0.603, total=   0.0s\n",
            "[CV] C=1, penalty=l2 .................................................\n",
            "[CV] ..................... C=1, penalty=l2, score=0.597, total=   0.0s\n",
            "[CV] C=1, penalty=l2 .................................................\n",
            "[CV] ..................... C=1, penalty=l2, score=0.602, total=   0.0s\n",
            "[CV] C=1, penalty=l2 .................................................\n",
            "[CV] ..................... C=1, penalty=l2, score=0.585, total=   0.0s\n",
            "[CV] C=10, penalty=l1 ................................................\n",
            "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=10, penalty=l1 ................................................\n",
            "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=10, penalty=l1 ................................................\n",
            "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=10, penalty=l1 ................................................\n",
            "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=10, penalty=l1 ................................................\n",
            "[CV] ...................... C=10, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=10, penalty=l2 ................................................\n",
            "[CV] .................... C=10, penalty=l2, score=0.633, total=   0.1s\n",
            "[CV] C=10, penalty=l2 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .................... C=10, penalty=l2, score=0.619, total=   0.1s\n",
            "[CV] C=10, penalty=l2 ................................................\n",
            "[CV] .................... C=10, penalty=l2, score=0.592, total=   0.1s\n",
            "[CV] C=10, penalty=l2 ................................................\n",
            "[CV] .................... C=10, penalty=l2, score=0.602, total=   0.1s\n",
            "[CV] C=10, penalty=l2 ................................................\n",
            "[CV] .................... C=10, penalty=l2, score=0.624, total=   0.1s\n",
            "[CV] C=100, penalty=l1 ...............................................\n",
            "[CV] ..................... C=100, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=100, penalty=l1 ...............................................\n",
            "[CV] ..................... C=100, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=100, penalty=l1 ...............................................\n",
            "[CV] ..................... C=100, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=100, penalty=l1 ...............................................\n",
            "[CV] ..................... C=100, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=100, penalty=l1 ...............................................\n",
            "[CV] ..................... C=100, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=100, penalty=l2 ...............................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................... C=100, penalty=l2, score=0.653, total=   0.1s\n",
            "[CV] C=100, penalty=l2 ...............................................\n",
            "[CV] ................... C=100, penalty=l2, score=0.636, total=   0.1s\n",
            "[CV] C=100, penalty=l2 ...............................................\n",
            "[CV] ................... C=100, penalty=l2, score=0.647, total=   0.1s\n",
            "[CV] C=100, penalty=l2 ..............................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[CV] ................... C=100, penalty=l2, score=0.627, total=   0.1s\n",
            "[CV] C=100, penalty=l2 ...............................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................... C=100, penalty=l2, score=0.635, total=   0.1s\n",
            "[CV] C=1000, penalty=l1 ..............................................\n",
            "[CV] .................... C=1000, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1000, penalty=l1 ..............................................\n",
            "[CV] .................... C=1000, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1000, penalty=l1 ..............................................\n",
            "[CV] .................... C=1000, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1000, penalty=l1 ..............................................\n",
            "[CV] .................... C=1000, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1000, penalty=l1 ..............................................\n",
            "[CV] .................... C=1000, penalty=l1, score=nan, total=   0.0s\n",
            "[CV] C=1000, penalty=l2 ..............................................\n",
            "[CV] .................. C=1000, penalty=l2, score=0.633, total=   0.1s\n",
            "[CV] C=1000, penalty=l2 ..............................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .................. C=1000, penalty=l2, score=0.631, total=   0.1s\n",
            "[CV] C=1000, penalty=l2 ..............................................\n",
            "[CV] .................. C=1000, penalty=l2, score=0.636, total=   0.1s\n",
            "[CV] C=1000, penalty=l2 ..............................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .................. C=1000, penalty=l2, score=0.621, total=   0.1s\n",
            "[CV] C=1000, penalty=l2 ..............................................\n",
            "[CV] .................. C=1000, penalty=l2, score=0.641, total=   0.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:    2.2s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
              "                         'penalty': ['l1', 'l2']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1jXfMkg4yiS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "30742757-f99d-4113-b251-5f68fecb6ffd"
      },
      "source": [
        " \n",
        "# print best parameter after tuning \n",
        "print(grid.best_params_) \n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning \n",
        "print(grid.best_estimator_) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 100, 'penalty': 'l2'}\n",
            "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaWyD4f827qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a, b, m = training(words, 100, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgXkgIbw27qX",
        "colab_type": "code",
        "colab": {},
        "outputId": "847e45d5-ac9c-4b59-c4f7-4514b67e389b"
      },
      "source": [
        "a, b, m = training(words, 25, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.1291200397469217, pvalue=0.021067703429049758)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.08940294960245106, pvalue=0.10658638479417483)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7stUUnl27qc",
        "colab_type": "code",
        "colab": {},
        "outputId": "cd508d8e-c999-4840-a408-6c588ceab2e5"
      },
      "source": [
        "a, b, m = training(words, 70, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 70\n",
            "137677\n",
            "137677\n",
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.4816698925491659, pvalue=1.2637866509260601e-24)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.4541854408839662, pvalue=7.220722015911838e-23)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU8fj9b927qf",
        "colab_type": "code",
        "colab": {},
        "outputId": "26356ef3-1eed-444f-e495-df8fbca39711"
      },
      "source": [
        "a, b, m = training(words, 50, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 50\n",
            "137677\n",
            "137677\n",
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.48417099560410753, pvalue=6.717386867801877e-25)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.45432781253696747, pvalue=6.975634824701234e-23)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNELsuGH27qj",
        "colab_type": "code",
        "colab": {},
        "outputId": "cc297f89-8b80-4d0b-e2e4-44be5420dd8d"
      },
      "source": [
        "a, b, m = training(words, 150, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 150\n",
            "137677\n",
            "137677\n",
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.5096595538031236, pvalue=7.895418909344523e-28)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.48077578757408546, pvalue=8.55157938182282e-26)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz8oiNRx27qo",
        "colab_type": "code",
        "colab": {},
        "outputId": "93aa16db-e1d1-46fc-d9d2-0449567bf069"
      },
      "source": [
        "a, b, m = training(words, 250, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 250\n",
            "137677\n",
            "137677\n",
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.511479324157144, pvalue=4.771248116932873e-28)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.48851914499112725, pvalue=1.0728801855048895e-26)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2xr8wNd27qq",
        "colab_type": "code",
        "colab": {},
        "outputId": "634ef0eb-0f73-440e-913e-8ea0def605d0"
      },
      "source": [
        "a, b, m = training(words, 550, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 550\n",
            "137677\n",
            "137677\n",
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.5156264787704207, pvalue=1.4968743528491977e-28)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.4947283291322275, pvalue=1.9538239363156295e-27)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKO6lkPV27qx",
        "colab_type": "code",
        "colab": {},
        "outputId": "5d8f43fd-8006-4eba-dd05-07b05a539741"
      },
      "source": [
        "a, b, m = training(words, 1000, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 1000\n",
            "137677\n",
            "137677\n",
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.516083325782594, pvalue=1.316141725809059e-28)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.49958248025018603, pvalue=5.035120126054427e-28)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzV9RC_x27q1",
        "colab_type": "code",
        "colab": {},
        "outputId": "a7792952-d98c-4297-989e-1b34117727a7"
      },
      "source": [
        "a, b, m = training(words, 2000, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 2000\n",
            "137677\n",
            "137677\n",
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.5206364451995322, pvalue=3.6116837955897164e-29)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.5078675869533443, pvalue=4.7302161364263977e-29)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fOJjCnB27q3",
        "colab_type": "code",
        "colab": {},
        "outputId": "371e954b-7729-48a0-9144-eb6f4bf6a9f8"
      },
      "source": [
        "a, b, m = training(words, 3000, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For size = 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/vanshika/anaconda3/envs/powerai_3.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result of UMNSRS-Sim data SpearmanrResult(correlation=0.09418179455657628, pvalue=0.0930990853888878)\n",
            "Result of UMNSRS-Rel data SpearmanrResult(correlation=0.1025049825694829, pvalue=0.06411297389515487)\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VIvd9z627q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}